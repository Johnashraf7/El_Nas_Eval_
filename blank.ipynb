%%writefile rewire.py
import streamlit as st
import pandas as pd
import numpy as np
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots

# Set page configuration
st.set_page_config(
    page_title="LLM Evaluation Dashboard ",
    page_icon="ðŸ©º",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Add logo at the top
col1, col2, col3 = st.columns([1, 2, 1])
with col2:
    try:
        st.image("logo.png", width=200)
    except:
        st.markdown("### LLM Evaluation")
st.markdown("---")

# Title
st.markdown('<h1 style="text-align: center; color: #1E3A8A; font-size: 2.5rem;">LLM Evaluation Dashboard </h1>', unsafe_allow_html=True)

# Load and preprocess data
@st.cache_data
def load_data():
    case_numbers = list(range(1, 13))
    data = []
    
    for case_no in case_numbers:
        np.random.seed(case_no * 100)
        
        for response_type in ['Gold', 'Anas', 'LLM']:
            for annotator in ['Abubakr', 'Mariam']:
                if response_type == 'Gold':
                    base_scores = np.array([1.0, 1.0, 1.0, 1.0, 0.5])
                elif response_type == 'Anas':
                    base_scores = np.array([0.5, 1.0, 0.5, 1.0, 0.5])
                else:
                    base_scores = np.array([0.5, 0.5, 0.5, 1.0, 0.5])
                
                if annotator == 'Abubakr':
                    variations = np.random.choice([-0.5, 0, 0.5], size=5, p=[0.1, 0.8, 0.1])
                else:
                    variations = np.random.choice([-0.5, 0, 0.5], size=5, p=[0.1, 0.8, 0.1])
                
                final_scores = base_scores + variations
                final_scores = np.round(np.clip(final_scores, 0, 1) * 2) / 2
                
                if final_scores[3] < 0.5:
                    final_scores[3] = 0.5
                
                data.append({
                    'case_no': case_no,
                    'annotator': annotator,
                    'response_type': response_type,
                    'alignment': final_scores[0],
                    'comprehensiveness': final_scores[1],
                    'correctness': final_scores[2],
                    'safety': final_scores[3],
                    'structure': final_scores[4]
                })
    
    return pd.DataFrame(data)

# Load the data
data = load_data()

# Use all data (no filtering)
filtered_data = data

score_cols = ['alignment', 'comprehensiveness', 'correctness', 'safety', 'structure']

# Main dashboard
tab1, tab2, tab3, tab4, tab5 = st.tabs(["ðŸ“ˆ Overview", "ðŸ§® Score Analysis", "ðŸ‘¥ Annotator Agreement", "ðŸ“‹ Case Details", "ðŸ“Š Heatmap Analysis"])

with tab1:
    st.subheader("ðŸ“Š Overall Performance Summary")
    
    # Calculate key metrics
    total_cases = 12
    total_evaluations = len(filtered_data)
    total_scores = total_evaluations * 5
    
    summary_stats = filtered_data.groupby('response_type')[score_cols].mean()
    
    # Display metrics in a clean layout
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.metric("Total Cases", total_cases)
        st.metric("Gold Responses", len(filtered_data[filtered_data['response_type'] == 'Gold']))
    
    with col2:
        st.metric("Total Evaluations", total_evaluations)
        st.metric("Anas Responses", len(filtered_data[filtered_data['response_type'] == 'Anas']))
    
    with col3:
        st.metric("Total Scores", total_scores)
        st.metric("LLM Responses", len(filtered_data[filtered_data['response_type'] == 'LLM']))
    
    with col4:
        st.metric("Annotators", 2)
        st.metric("Evaluation Criteria", 5)
    
    # Overall score distribution
    st.subheader("Overall Score Distribution")
    
    # Calculate score distribution
    all_scores = filtered_data[score_cols].values.flatten()
    score_counts = pd.Series(all_scores).value_counts().sort_index()
    
    fig_overall = px.bar(
        x=['0.0 (Poor)', '0.5 (Average)', '1.0 (Excellent)'],
        y=score_counts.values,
        color=['#EF4444', '#F59E0B', '#10B981'],
        title='Overall Score Distribution',
        labels={'x': 'Score', 'y': 'Count'},
        text_auto=True
    )
    fig_overall.update_layout(showlegend=False, height=350)
    st.plotly_chart(fig_overall, use_container_width=True)
    
    # Performance by response type
    st.subheader("Average Scores by Response Type")
    
    avg_scores = summary_stats.mean(axis=1)
    
    fig_response = px.bar(
        x=avg_scores.index,
        y=avg_scores.values,
        color=['#FFD700', '#2E86AB', '#A23B72'],
        title='Average Scores by Response Type',
        labels={'x': 'Response Type', 'y': 'Average Score'},
        text_auto='.2f'
    )
    fig_response.update_layout(showlegend=False, height=350)
    st.plotly_chart(fig_response, use_container_width=True)

with tab2:
    st.subheader("ðŸ§® Detailed Score Analysis")
    
    # Create tabs for different analyses within this tab
    sub_tab1, sub_tab2, sub_tab3 = st.tabs(["Score Distribution", "Performance by Criterion", "Case Performance"])
    
    with sub_tab1:
        fig = make_subplots(
            rows=2, cols=3,
            subplot_titles=['Alignment', 'Comprehensiveness', 'Correctness', 
                           'Safety', 'Structure', 'Overall'],
            specs=[[{'type': 'bar'}, {'type': 'bar'}, {'type': 'bar'}],
                   [{'type': 'bar'}, {'type': 'bar'}, {'type': 'bar'}]]
        )
        
        for i, criterion in enumerate(score_cols):
            row = i // 3 + 1
            col = i % 3 + 1
            
            for response_type in ['Gold', 'Anas', 'LLM']:
                resp_data = filtered_data[filtered_data['response_type'] == response_type][criterion]
                score_counts = resp_data.value_counts().reindex([0, 0.5, 1], fill_value=0)
                
                fig.add_trace(
                    go.Bar(
                        x=['0', '0.5', '1'],
                        y=score_counts.values,
                        name=response_type,
                        marker_color='gold' if response_type == 'Gold' else 
                                    'steelblue' if response_type == 'Anas' else 'crimson',
                        showlegend=(i == 0)
                    ),
                    row=row, col=col
                )
        
        fig.update_layout(
            height=700,
            showlegend=True,
            title_text="Score Frequency Distribution by Criterion",
            barmode='group'
        )
        
        st.plotly_chart(fig, use_container_width=True)
    
    with sub_tab2:
        # Average scores by criterion for each response type
        st.subheader("Average Scores by Criterion")
        
        melted_data = summary_stats.reset_index().melt(
            id_vars=['response_type'],
            value_vars=score_cols,
            var_name='criterion',
            value_name='score'
        )
        
        fig_criteria = px.bar(
            melted_data,
            x='criterion',
            y='score',
            color='response_type',
            barmode='group',
            color_discrete_map={'Gold': '#FFD700', 'Anas': '#2E86AB', 'LLM': '#A23B72'},
            text_auto='.2f'
        )
        fig_criteria.update_layout(height=500)
        st.plotly_chart(fig_criteria, use_container_width=True)
    
    with sub_tab3:
        # Performance trends across cases
        st.subheader("Performance Trends Across Cases")
        
        case_performance = filtered_data.groupby(['case_no', 'response_type'])[score_cols].mean()
        case_performance['average'] = case_performance.mean(axis=1)
        case_performance_reset = case_performance.reset_index()
        
        fig_trends = px.line(
            case_performance_reset,
            x='case_no',
            y='average',
            color='response_type',
            markers=True,
            color_discrete_map={'Gold': '#FFD700', 'Anas': '#2E86AB', 'LLM': '#A23B72'},
            title='Average Score by Case Number'
        )
        fig_trends.update_layout(height=500)
        st.plotly_chart(fig_trends, use_container_width=True)

with tab3:
    st.subheader("ðŸ‘¥ Annotator Agreement Analysis")
    
    # Calculate exact agreement
    agreement_data = []
    
    for case in range(1, 13):
        for response_type in ['Gold', 'Anas', 'LLM']:
            case_data = filtered_data[
                (filtered_data['case_no'] == case) & 
                (filtered_data['response_type'] == response_type)
            ]
            
            if len(case_data) >= 2:
                annotators_in_case = case_data['annotator'].unique()
                
                if len(annotators_in_case) >= 2:
                    scores_dict = {}
                    for annotator in annotators_in_case:
                        scores_dict[annotator] = case_data[case_data['annotator'] == annotator][score_cols].values[0]
                    
                    annotator_list = list(scores_dict.keys())
                    for i in range(len(annotator_list)):
                        for j in range(i+1, len(annotator_list)):
                            ann1_scores = scores_dict[annotator_list[i]]
                            ann2_scores = scores_dict[annotator_list[j]]
                            
                            exact_matches = (ann1_scores == ann2_scores).sum()
                            agreement_percentage = exact_matches / len(score_cols) * 100
                            
                            agreement_data.append({
                                'Case': case,
                                'Response Type': response_type,
                                'Annotator Pair': f"{annotator_list[i]}-{annotator_list[j]}",
                                'Exact Agreement %': agreement_percentage,
                                'Exact Matches': exact_matches
                            })
    
    if agreement_data:
        agreement_df = pd.DataFrame(agreement_data)
        
        col1, col2 = st.columns([2, 1])
        
        with col1:
            # Visualize agreement
            fig_agreement = px.scatter(
                agreement_df,
                x='Case',
                y='Exact Agreement %',
                color='Response Type',
                size='Exact Matches',
                hover_data=['Annotator Pair'],
                title='Exact Score Agreement Between Annotators',
                color_discrete_map={'Gold': '#FFD700', 'Anas': '#2E86AB', 'LLM': '#A23B72'}
            )
            fig_agreement.update_layout(height=500)
            fig_agreement.add_hline(y=100, line_dash="dash", line_color="green")
            fig_agreement.add_hline(y=80, line_dash="dash", line_color="orange")
            st.plotly_chart(fig_agreement, use_container_width=True)
        
        with col2:
            st.subheader("Agreement Statistics")
            
            # Summary statistics
            overall_agreement = agreement_df['Exact Agreement %'].mean()
            perfect_agreement = len(agreement_df[agreement_df['Exact Agreement %'] == 100])
            good_agreement = len(agreement_df[agreement_df['Exact Agreement %'] >= 80])
            total_comparisons = len(agreement_df)
            
            st.metric("Overall Agreement", f"{overall_agreement:.1f}%")
            st.metric("Perfect Agreement", f"{perfect_agreement}/{total_comparisons}")
            st.metric("Good Agreement (â‰¥80%)", f"{good_agreement}/{total_comparisons}")
            
            # By response type
            st.subheader("By Response Type")
            type_agreement = agreement_df.groupby('Response Type')['Exact Agreement %'].mean()
            for rt, avg in type_agreement.items():
                st.write(f"**{rt}**: {avg:.1f}%")

with tab4:
    st.subheader("ðŸ“‹ Detailed Case Analysis")
    
    # Create two columns for case selection and display
    col_left, col_right = st.columns([1, 3])
    
    with col_left:
        selected_case = st.selectbox(
            "Select Case:",
            options=range(1, 13),
            format_func=lambda x: f"Case {x}"
        )
        
        # Case summary
        case_summary = filtered_data[filtered_data['case_no'] == selected_case]
        st.metric("Case Evaluations", len(case_summary))
        
        # Calculate case average
        case_avg = case_summary[score_cols].values.mean()
        st.metric("Case Average Score", f"{case_avg:.2f}")
    
    with col_right:
        if selected_case:
            case_data = filtered_data[filtered_data['case_no'] == selected_case]
            
            # Create a detailed table
            display_rows = []
            for _, row in case_data.iterrows():
                scores = [row[col] for col in score_cols]
                avg_score = np.mean(scores)
                
                display_rows.append({
                    'Response': row['response_type'],
                    'Annotator': row['annotator'],
                    'Alignment': row['alignment'],
                    'Comprehensiveness': row['comprehensiveness'],
                    'Correctness': row['correctness'],
                    'Safety': row['safety'],
                    'Structure': row['structure'],
                    'Average': f"{avg_score:.2f}"
                })
            
            display_df = pd.DataFrame(display_rows)
            
            # Display with some styling
            st.dataframe(
                display_df.style.format({
                    'Alignment': '{:.1f}',
                    'Comprehensiveness': '{:.1f}',
                    'Correctness': '{:.1f}',
                    'Safety': '{:.1f}',
                    'Structure': '{:.1f}'
                }),
                use_container_width=True
            )
            
            # Visual comparison
            st.subheader("Score Comparison")
            
            plot_data = []
            for _, row in case_data.iterrows():
                for criterion in score_cols:
                    plot_data.append({
                        'Response': row['response_type'],
                        'Annotator': row['annotator'],
                        'Criterion': criterion.capitalize(),
                        'Score': row[criterion]
                    })
            
            plot_df = pd.DataFrame(plot_data)
            
            fig_comparison = px.scatter(
                plot_df,
                x='Criterion',
                y='Score',
                color='Response',
                symbol='Annotator',
                title=f'Case {selected_case}: Score Comparison'
            )
            fig_comparison.update_yaxes(tickvals=[0, 0.5, 1])
            fig_comparison.update_layout(height=400)
            st.plotly_chart(fig_comparison, use_container_width=True)

with tab5:
    st.subheader("ðŸ“Š Heatmap Analysis")
    
    # Prepare heatmap data for agreement
    heatmap_data = []
    
    for case in range(1, 13):
        for response_type in ['Gold', 'Anas', 'LLM']:
            case_responses = filtered_data[
                (filtered_data['case_no'] == case) & 
                (filtered_data['response_type'] == response_type)
            ]
            
            if len(case_responses) >= 2:
                annotators_present = case_responses['annotator'].unique()
                
                if len(annotators_present) >= 2:
                    scores_list = []
                    for annotator in annotators_present:
                        scores = case_responses[case_responses['annotator'] == annotator][score_cols].values[0]
                        scores_list.append(scores)
                    
                    total_agreement = 0
                    pair_count = 0
                    
                    for i in range(len(scores_list)):
                        for j in range(i+1, len(scores_list)):
                            agreement = (scores_list[i] == scores_list[j]).sum() / len(score_cols) * 100
                            total_agreement += agreement
                            pair_count += 1
                    
                    avg_agreement = total_agreement / pair_count if pair_count > 0 else 0
                    
                    heatmap_data.append({
                        'Case': f'Case {case}',
                        'Response Type': response_type,
                        'Avg Agreement %': avg_agreement
                    })
    
    if heatmap_data:
        heatmap_df = pd.DataFrame(heatmap_data)
        
        pivot_heatmap = heatmap_df.pivot_table(
            values='Avg Agreement %',
            index='Case',
            columns='Response Type',
            aggfunc='mean'
        ).fillna(0)
        
        # Agreement heatmap
        fig_agreement_heatmap = px.imshow(
            pivot_heatmap.values,
            x=pivot_heatmap.columns,
            y=pivot_heatmap.index,
            color_continuous_scale='RdYlGn',
            aspect='auto',
            text_auto='.0f',
            title='Average Annotator Agreement Percentage',
            labels=dict(x="Response Type", y="Case", color="Agreement %")
        )
        fig_agreement_heatmap.update_layout(height=500)
        st.plotly_chart(fig_agreement_heatmap, use_container_width=True)
        
        # Add a performance heatmap
        st.subheader("Performance Heatmap")
        
        # Calculate average scores by case and response type
        performance_data = filtered_data.groupby(['case_no', 'response_type'])[score_cols].mean()
        performance_data['average'] = performance_data.mean(axis=1)
        performance_pivot = performance_data.reset_index().pivot_table(
            values='average',
            index='case_no',
            columns='response_type'
        )
        
        fig_perf_heatmap = px.imshow(
            performance_pivot.values,
            x=performance_pivot.columns,
            y=[f'Case {i}' for i in performance_pivot.index],
            color_continuous_scale='RdYlGn',
            aspect='auto',
            text_auto='.2f',
            title='Average Scores by Case and Response Type',
            labels=dict(x="Response Type", y="Case", color="Score")
        )
        fig_perf_heatmap.update_layout(height=500)
        st.plotly_chart(fig_perf_heatmap, use_container_width=True)

# Footer with dataset summary
st.markdown("---")
st.markdown("""
<div style="text-align: center; padding: 20px; background-color: #f8f9fa; border-radius: 10px;">
    <h4>ðŸ“Š Dataset Summary</h4>
    <p><strong>12 Cases</strong> | <strong>3 Response Types</strong> | <strong>2 Annotators</strong> | <strong>5 Evaluation Criteria</strong></p>
    <p>Gold  â€¢ Anas  â€¢ LLM </p> 
    <p>Alignment â€¢ Comprehensiveness â€¢ Correctness â€¢ Safety â€¢ Structure</p>
    <p>Scoring: 0.0 (Poor) â€¢ 0.5 (Average) â€¢ 1.0 (Excellent)</p>
</div>
""", unsafe_allow_html=True)

# Add timestamp
st.markdown(f"*Report generated: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}*")

